{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# 为了简化，这里我们直接使用原始代码中的ConjunctionSet类\n",
    "from flextrees.utils.ConjunctionSet import ConjunctionSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "N_CLIENTS = 5  # 客户端数量\n",
    "DATA_DISTRIBUTION = 'iid'  # 'iid' 或 'non-iid'\n",
    "MODEL_TYPE = 'cart'  # 模型类型，简化版只实现'cart'\n",
    "MAX_DEPTH = 5  # 决策树最大深度\n",
    "    \n",
    "# 筛选参数\n",
    "FILTERING_METHOD = 'mean'  # 筛选方法\n",
    "ACC_THRESHOLD = 0.6  # 准确率阈值\n",
    "F1_THRESHOLD = 0.5  # F1分数阈值\n",
    "\n",
    "# 3. 配置本地模型参数\n",
    "local_model_params = {\n",
    "    'max_depth': MAX_DEPTH,\n",
    "    'criterion': 'gini',\n",
    "    'splitter': 'best',\n",
    "    'model_type': MODEL_TYPE,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "加载数据集...\n",
      "数据集加载完成，特征数量: 109\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(dataset_name='adult', categorical=False):\n",
    "    \"\"\"加载指定的数据集\"\"\"\n",
    "    try:\n",
    "        # 这里简化为手动加载adult数据集\n",
    "        from flextrees.datasets import adult\n",
    "        return adult(ret_feature_names=True, categorical=categorical)\n",
    "    except ImportError:\n",
    "        # 如果找不到特定的数据集，使用模拟数据\n",
    "        print(\"未找到指定数据集，使用模拟数据\")\n",
    "        from sklearn.datasets import make_classification\n",
    "        X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
    "        feature_names = [f'x{i}' for i in range(X.shape[1])]\n",
    "        X_df = pd.DataFrame(X, columns=feature_names)\n",
    "        y_df = pd.Series(y)\n",
    "        \n",
    "        # 创建数据集对象\n",
    "        class Dataset:\n",
    "            def __init__(self, X, y):\n",
    "                self.X_data = X\n",
    "                self.y_data = y\n",
    "            def to_numpy(self):\n",
    "                return self.X_data.to_numpy(), self.y_data.to_numpy()\n",
    "                \n",
    "        train_data = Dataset(X_df[:800], y_df[:800])\n",
    "        test_data = Dataset(X_df[800:], y_df[800:])\n",
    "        return train_data, test_data, feature_names\n",
    "\n",
    "# 1. 加载数据\n",
    "print(f\"\\n加载数据集...\")\n",
    "train_data, test_data, feature_names = load_dataset(categorical=False)\n",
    "print(f\"数据集加载完成，特征数量: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "将数据分发到 5 个客户端 (iid 分布)...\n"
     ]
    }
   ],
   "source": [
    "def split_data_to_clients(data, n_clients=5, iid=True):\n",
    "    \"\"\"将数据分割给多个客户端\"\"\"\n",
    "    X, y = data.X_data.to_numpy(), data.y_data.to_numpy()\n",
    "    client_data = []\n",
    "    \n",
    "    if iid:\n",
    "        # IID分割: 随机均匀分割\n",
    "        indices = np.random.permutation(len(X))\n",
    "        chunk_size = len(indices) // n_clients\n",
    "        \n",
    "        for i in range(n_clients):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = (i + 1) * chunk_size if i < n_clients - 1 else len(indices)\n",
    "            client_indices = indices[start_idx:end_idx]\n",
    "            \n",
    "            # 创建数据集对象\n",
    "            class ClientDataset:\n",
    "                def __init__(self, X, y):\n",
    "                    self.X_data = pd.DataFrame(X)\n",
    "                    self.y_data = pd.Series(y)\n",
    "            \n",
    "            client_data.append(ClientDataset(X[client_indices], y[client_indices]))\n",
    "    else:\n",
    "        # Non-IID分割: 按类别偏向分配\n",
    "        classes = np.unique(y)\n",
    "        client_indices = [[] for _ in range(n_clients)]\n",
    "        \n",
    "        # 按类别分配\n",
    "        for c in classes:\n",
    "            idx = np.where(y == c)[0]\n",
    "            np.random.shuffle(idx)\n",
    "            \n",
    "            # 偏向分配\n",
    "            if len(classes) >= n_clients:\n",
    "                # 如果类别数多于客户端数，每个客户端主要分配一种类型\n",
    "                client_id = int(c % n_clients)\n",
    "                client_indices[client_id].extend(idx[:int(len(idx)*0.6)])\n",
    "                \n",
    "                # 其余的随机分配\n",
    "                remaining_idx = idx[int(len(idx)*0.6):]\n",
    "                np.random.shuffle(remaining_idx)\n",
    "                chunk_size = len(remaining_idx) // n_clients\n",
    "                for i in range(n_clients):\n",
    "                    start_idx = i * chunk_size\n",
    "                    end_idx = (i + 1) * chunk_size if i < n_clients - 1 else len(remaining_idx)\n",
    "                    client_indices[i].extend(remaining_idx[start_idx:end_idx])\n",
    "            else:\n",
    "                # 如果类别数少于客户端数，将每个类别平均分配\n",
    "                chunk_size = len(idx) // n_clients\n",
    "                for i in range(n_clients):\n",
    "                    start_idx = i * chunk_size\n",
    "                    end_idx = (i + 1) * chunk_size if i < n_clients - 1 else len(idx)\n",
    "                    client_indices[i].extend(idx[start_idx:end_idx])\n",
    "        \n",
    "        # 创建数据集\n",
    "        for indices in client_indices:\n",
    "            class ClientDataset:\n",
    "                def __init__(self, X, y):\n",
    "                    self.X_data = pd.DataFrame(X)\n",
    "                    self.y_data = pd.Series(y)\n",
    "            \n",
    "            client_data.append(ClientDataset(X[indices], y[indices]))\n",
    "    \n",
    "    return client_data\n",
    "\n",
    "    \n",
    "# 2. 将数据分发到客户端\n",
    "print(f\"\\n将数据分发到 {N_CLIENTS} 个客户端 ({DATA_DISTRIBUTION} 分布)...\")\n",
    "client_data = split_data_to_clients(train_data, N_CLIENTS, DATA_DISTRIBUTION == 'iid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 树训练和规则提取函数\n",
    "def train_local_model(client_data, model_params):\n",
    "    \"\"\"在本地训练决策树并提取规则\"\"\"\n",
    "    # 根据模型类型创建分类器\n",
    "    model_type = model_params.get('model_type', 'cart')\n",
    "    \n",
    "    # 创建分类器，这里简化只使用CART决策树\n",
    "    clf = DecisionTreeClassifier(\n",
    "        random_state=42,\n",
    "        min_samples_split=max(1.0, int(0.02 * len(client_data.X_data))),\n",
    "        max_depth=model_params.get('max_depth', 5),\n",
    "        criterion=model_params.get('criterion', 'gini'),\n",
    "        splitter=model_params.get('splitter', 'best')\n",
    "    )\n",
    "    \n",
    "    # 准备训练数据\n",
    "    X_data, y_data = client_data.X_data.to_numpy(), client_data.y_data.to_numpy()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # 训练模型\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # 模型评估\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    print(f\"客户端本地模型 - 准确率: {acc:.4f}, F1分数: {f1:.4f}, 训练样本数: {len(X_train)}, 测试样本数: {len(X_test)}\")\n",
    "    \n",
    "    # 从决策树提取规则\n",
    "    feature_names = [f'x{i}' for i in range(X_data.shape[1])]\n",
    "    feature_types = ['int'] * len(feature_names)\n",
    "    \n",
    "    # 创建规则集合\n",
    "    local_cs = ConjunctionSet(\n",
    "        feature_names=feature_names, \n",
    "        original_data=X_train, \n",
    "        pruning_x=X_train, \n",
    "        pruning_y=y_train,\n",
    "        model=[clf],  # 模型列表\n",
    "        feature_types=feature_types,  # 特征类型\n",
    "        amount_of_branches_threshold=3000,  # 分支数量阈值\n",
    "        minimal_forest_size=1,  # 最小森林大小\n",
    "        estimators=clf,  # 估计器\n",
    "        filter_approach='probability',  # 过滤方法\n",
    "        personalized=False  # 是否个性化\n",
    "    )\n",
    "    \n",
    "    # 创建返回结果\n",
    "    result = {\n",
    "        'local_tree': clf,\n",
    "        'local_cs': local_cs,\n",
    "        'local_branches': local_cs.get_branches_list(),\n",
    "        'local_branches_df': local_cs.get_conjunction_set_df().round(decimals=5),\n",
    "        'local_classes': clf.classes_,\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test,\n",
    "        'local_acc': acc,\n",
    "        'local_f1': f1\n",
    "    }\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第1步: 训练本地模型并提取规则...\n",
      "\n",
      "客户端1训练中...\n",
      "客户端本地模型 - 准确率: 0.8564, F1分数: 0.7827, 训练样本数: 3646, 测试样本数: 912\n",
      "Estimators: DecisionTreeClassifier(max_depth=5, min_samples_split=91, random_state=42)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AQUÍr\n",
      "\n",
      "客户端2训练中...\n",
      "客户端本地模型 - 准确率: 0.8410, F1分数: 0.7131, 训练样本数: 3646, 测试样本数: 912\n",
      "Estimators: DecisionTreeClassifier(max_depth=5, min_samples_split=91, random_state=42)\n",
      "AQUÍr\n",
      "\n",
      "客户端3训练中...\n",
      "客户端本地模型 - 准确率: 0.8531, F1分数: 0.7621, 训练样本数: 3646, 测试样本数: 912\n",
      "Estimators: DecisionTreeClassifier(max_depth=5, min_samples_split=91, random_state=42)\n",
      "AQUÍr\n",
      "\n",
      "客户端4训练中...\n",
      "客户端本地模型 - 准确率: 0.8509, F1分数: 0.7722, 训练样本数: 3646, 测试样本数: 912\n",
      "Estimators: DecisionTreeClassifier(max_depth=5, min_samples_split=91, random_state=42)\n",
      "AQUÍr\n",
      "\n",
      "客户端5训练中...\n",
      "客户端本地模型 - 准确率: 0.8432, F1分数: 0.7868, 训练样本数: 3648, 测试样本数: 912\n",
      "Estimators: DecisionTreeClassifier(max_depth=5, min_samples_split=91, random_state=42)\n",
      "AQUÍr\n",
      "第1步: 本地模型训练完成，规则已提取\n"
     ]
    }
   ],
   "source": [
    "# 4. 在每个客户端训练本地模型并提取规则\n",
    "print(\"\\n第1步: 训练本地模型并提取规则...\")\n",
    "client_models = []\n",
    "for i, data in enumerate(client_data):\n",
    "    print(f\"\\n客户端{i+1}训练中...\")\n",
    "    model = train_local_model(data, local_model_params)\n",
    "    client_models.append(model)\n",
    "print(\"第1步: 本地模型训练完成，规则已提取\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第2-5步: 筛选弱决策树...\n",
      "客户端 1 正在评估所有树...\n",
      "客户端 2 正在评估所有树...\n",
      "客户端 3 正在评估所有树...\n",
      "客户端 4 正在评估所有树...\n",
      "客户端 5 正在评估所有树...\n",
      "筛选后选择了 3 棵树，索引: [2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# 3. 规则评估和筛选函数\n",
    "def evaluate_trees_on_client(client_model, all_models):\n",
    "    \"\"\"评估所有模型在当前客户端数据上的表现\"\"\"\n",
    "    X_test, y_test = client_model['X_test'], client_model['y_test']\n",
    "    eval_results = []\n",
    "    \n",
    "    for model in all_models:\n",
    "        tree = model['local_tree']\n",
    "        y_pred = tree.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        eval_results.append((acc, f1))\n",
    "    \n",
    "    return eval_results\n",
    "\n",
    "def filter_trees(evaluation_results, filter_params):\n",
    "    \"\"\"基于评估结果筛选树\"\"\"\n",
    "    # 计算平均性能\n",
    "    avg_results = np.mean(evaluation_results, axis=0)\n",
    "    \n",
    "    # 根据筛选方法确定阈值\n",
    "    filter_method = filter_params.get('filter_method', 'mean')\n",
    "    if filter_method == 'mean':\n",
    "        acc_threshold = np.mean(avg_results[:,0])\n",
    "        f1_threshold = np.mean(avg_results[:,1])\n",
    "    elif filter_method == 'percentile':\n",
    "        percentile_value = filter_params.get('filter_value', 75)\n",
    "        acc_threshold = np.percentile(avg_results[:,0], percentile_value)\n",
    "        f1_threshold = np.percentile(avg_results[:,1], percentile_value)\n",
    "    else:\n",
    "        # 默认使用固定阈值\n",
    "        acc_threshold = filter_params.get('acc_threshold', 0.6)\n",
    "        f1_threshold = filter_params.get('f1_threshold', 0.5)\n",
    "    \n",
    "    # 筛选满足条件的树索引\n",
    "    selected_indices = []\n",
    "    for i in range(len(avg_results)):\n",
    "        # print(avg_results[i], acc_threshold, f1_threshold)\n",
    "        if avg_results[i][0] >= acc_threshold and avg_results[i][1] >= f1_threshold:\n",
    "            selected_indices.append(i)\n",
    "    \n",
    "    # 如果没有树被选中，选择表现最好的一棵\n",
    "    if not selected_indices:\n",
    "        best_idx = np.argmax(avg_results[0])  # 使用准确率选择\n",
    "        selected_indices = [best_idx]\n",
    "    \n",
    "    print(f\"筛选后选择了 {len(selected_indices)} 棵树，索引: {selected_indices}\")\n",
    "    return selected_indices\n",
    "\n",
    "\n",
    "# 5. 评估所有客户端上的所有树\n",
    "print(\"\\n第2-5步: 筛选弱决策树...\")\n",
    "all_evaluations = []\n",
    "for i, client_model in enumerate(client_models):\n",
    "    print(f\"客户端 {i+1} 正在评估所有树...\")\n",
    "    eval_results = evaluate_trees_on_client(client_model, client_models)\n",
    "    all_evaluations.append(eval_results)\n",
    "    \n",
    "# 6. 筛选表现好的树\n",
    "filter_params = {\n",
    "    'filter_method': FILTERING_METHOD,\n",
    "    'acc_threshold': ACC_THRESHOLD / 2, \n",
    "    'f1_threshold': F1_THRESHOLD / 2\n",
    "}\n",
    "    \n",
    "selected_indices = filter_trees(all_evaluations, filter_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cs_dt_branches_from_list(client_cs, classes_, tree_model, threshold=3000):\n",
    "    \"\"\"Function that generate a global ConjuctionSet, a GlobalTree and the branches\n",
    "    associated to the tree in the server node.\n",
    "    \"\"\"\n",
    "\n",
    "    cs = ConjunctionSet(\n",
    "        filter_approach=\"entropy\",\n",
    "        amount_of_branches_threshold=threshold,\n",
    "        feature_names=[],\n",
    "        personalized=False,\n",
    "    )\n",
    "    cs.aggregate_branches(client_cs, classes_)\n",
    "    cs.buildConjunctionSet()\n",
    "    print(f\"Conjunction set length: {len(cs.conjunctionSet)}\")\n",
    "    cs.conjunctionSet = delete_duplicated_rules(cs.conjunctionSet)\n",
    "    print(f\"Conjunction set length after removing duplicates: {len(cs.conjunctionSet)}\")\n",
    "    branches_df_aggregator = cs.get_conjunction_set_df().round(decimals=5)\n",
    "\n",
    "    probabilities = branches_df_aggregator[\"branch_probability\"].to_list()\n",
    "    # new_probas = [x for x in probabilities]\n",
    "    new_probas = list(probabilities)\n",
    "    total_probas = sum(new_probas)\n",
    "    # self._save_rules(client_cs, cs, round_number)\n",
    "    branches_df_aggregator[\"branch_probability\"] = branches_df_aggregator[\n",
    "        \"branch_probability\"\n",
    "    ].map(lambda x: x / total_probas)\n",
    "    if pd.isna(branches_df_aggregator).any().any():\n",
    "        import time\n",
    "        time.sleep(5)\n",
    "        print(\"Before fillna\")\n",
    "        print(branches_df_aggregator)\n",
    "        branches_df_aggregator = branches_df_aggregator.fillna(np.inf)\n",
    "        print(\"After fillna\")\n",
    "        print(branches_df_aggregator)\n",
    "        time.sleep(6)\n",
    "    else:\n",
    "        print(f\"branches df aggreagator is not null: {branches_df_aggregator}\")\n",
    "        branches_df_aggregator.to_csv(\"branches_df_aggregator.csv\")\n",
    "    new_df_dict = {\n",
    "        col: branches_df_aggregator[col].values\n",
    "        for col in branches_df_aggregator.columns\n",
    "    }\n",
    "    new_dt_model = tree_model([True] * len(branches_df_aggregator), classes_)\n",
    "    new_dt_model.split(new_df_dict)\n",
    "    return [cs, new_dt_model, branches_df_aggregator]\n",
    "\n",
    "\n",
    "def delete_duplicated_rules(rules_dataset):\n",
    "    rules = {str(rule): rule for rule in rules_dataset}\n",
    "    return list(rules.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "EPSILON = 0.000001\n",
    "\n",
    "#################################### NEW ####################################\n",
    "from sklearn.metrics import auc, cohen_kappa_score, roc_curve\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "\n",
    "class TreeBranch:\n",
    "    def __init__(self, mask, classes=None, depth=0):\n",
    "        self.mask = mask\n",
    "        # print(mask)\n",
    "        # print(self.mask)\n",
    "        self.classes_ = classes\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.split_feature = None\n",
    "        self.split_value = None\n",
    "        self.detph = depth\n",
    "\n",
    "    def split(self, df):\n",
    "        \"\"\"Function that splits a node into two childs.\n",
    "\n",
    "        Args:\n",
    "            df (Pandas.DataFrame): Dataframe with the instances of the node.\n",
    "        \"\"\"\n",
    "        # print(df)\n",
    "        # if np.sum(self.mask)==1 or self.has_same_class(df):\n",
    "        if np.sum(self.mask) == 1:\n",
    "            self.left = None\n",
    "            self.right = None\n",
    "            return\n",
    "        self.features = [int(i.split(\"_\")[0]) for i in df.keys() if \"upper\" in str(i)]\n",
    "        # print(self.features)\n",
    "        # print(f\"Printing self.mask: {self.mask}\")\n",
    "        # print(f\"Printing len self.mask: {len(self.mask)}\")\n",
    "        self.split_feature, self.split_value = self.select_split_feature(df)\n",
    "        self.create_mask(df)\n",
    "        is_splitable = self.is_splitable()\n",
    "        if is_splitable is False:\n",
    "            self.left = None\n",
    "            self.right = None\n",
    "            return\n",
    "        # print(f\"Left tree mask: {list(np.logical_and(self.mask,np.logical_or(self.left_mask,self.both_mask)))}\")\n",
    "        # print(f\"Left len tree mask: {len(list(np.logical_and(self.mask,np.logical_or(self.left_mask,self.both_mask))))}\")\n",
    "        # print(f\"Right tree mask: {list(np.logical_and(self.mask,np.logical_or(self.right_mask,self.both_mask)))}\")\n",
    "        # print(f\"Right len tree mask: {len(list(np.logical_and(self.mask,np.logical_or(self.right_mask,self.both_mask))))}\")\n",
    "        # print(f\"Both mask: {self.both_mask}\")\n",
    "        # print(f\"Len de both mask: {len(self.both_mask)}\")\n",
    "        # print(f\"Logical or entre right mask y both mask: {np.logical_or(self.right_mask,self.both_mask)}\")\n",
    "        # print(f\"True right mask: {self.right_mask}\")\n",
    "        self.left = TreeBranch(\n",
    "            list(\n",
    "                np.logical_and(self.mask, np.logical_or(self.left_mask, self.both_mask))\n",
    "            ),\n",
    "            self.classes_,\n",
    "            depth=self.detph + 1,\n",
    "        )\n",
    "        self.right = TreeBranch(\n",
    "            list(\n",
    "                np.logical_and(\n",
    "                    self.mask, np.logical_or(self.right_mask, self.both_mask)\n",
    "                )\n",
    "            ),\n",
    "            self.classes_,\n",
    "            depth=self.detph + 1,\n",
    "        )\n",
    "        self.left.split(df)\n",
    "        self.right.split(df)\n",
    "\n",
    "    def is_splitable(self):\n",
    "        \"\"\"Function that checks if a node is splittable.\n",
    "\n",
    "        Returns:\n",
    "            bool: Returns True if the node is splittable, False otherwise.\n",
    "        \"\"\"\n",
    "        if (\n",
    "            np.sum(\n",
    "                np.logical_and(self.mask, np.logical_or(self.left_mask, self.both_mask))\n",
    "            )\n",
    "            == 0\n",
    "            or np.sum(\n",
    "                np.logical_and(\n",
    "                    self.mask, np.logical_or(self.right_mask, self.both_mask)\n",
    "                )\n",
    "            )\n",
    "            == 0\n",
    "        ):\n",
    "            return False\n",
    "        if np.sum(\n",
    "            np.logical_and(self.mask, np.logical_or(self.left_mask, self.both_mask))\n",
    "        ) == np.sum(self.mask) or np.sum(\n",
    "            np.logical_and(self.mask, np.logical_or(self.right_mask, self.both_mask))\n",
    "        ) == np.sum(\n",
    "            self.mask\n",
    "        ):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def create_mask(self, df):\n",
    "        \"\"\"Function that creates the mask for the childs of a node.\n",
    "\n",
    "        Args:\n",
    "            df (Pandas.Dataframe): Dataframe with the instances of the node.\n",
    "        \"\"\"\n",
    "        self.left_mask = df[str(self.split_feature) + \"_upper\"] <= self.split_value\n",
    "        self.right_mask = df[str(self.split_feature) + \"_lower\"] >= self.split_value\n",
    "        self.both_mask = (df[str(self.split_feature) + \"_lower\"] < self.split_value) & (\n",
    "            df[str(self.split_feature) + \"_upper\"] > self.split_value\n",
    "        )\n",
    "        # self.both_mask = [True if self.split_value < upper and self.split_value > lower else False for lower, upper in\n",
    "        #             zip(df[str(self.split_feature) + '_lower'], df[str(self.split_feature) + \"_upper\"])]\n",
    "\n",
    "    def select_split_feature(self, df):\n",
    "        \"\"\"Function that select the feature to split the node. It calculates the\n",
    "        metric for each feature and returns the feature with the lowest metric.\n",
    "\n",
    "        Args:\n",
    "            df (Pandas.DataFrame): Dataframe with the instances of the node.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tuple containing the feature and the value of the feature that\n",
    "                minimizes the metric.\n",
    "        \"\"\"\n",
    "        feature_to_value = {}\n",
    "        feature_to_metric = {}\n",
    "        for feature in self.features:\n",
    "            value, metric = self.check_feature_split_value(df, feature)\n",
    "            feature_to_value[feature] = value\n",
    "            feature_to_metric[feature] = metric\n",
    "        # print('SELECT_SPLIT_FEATURE')\n",
    "        # print(feature_to_value)\n",
    "        # print(feature_to_metric)\n",
    "        feature = min(feature_to_metric, key=feature_to_metric.get)\n",
    "        return feature, feature_to_value[feature]\n",
    "\n",
    "    def check_feature_split_value(self, df, feature):\n",
    "        \"\"\"Function that calculate the metric for a given feature.\n",
    "\n",
    "        Args:\n",
    "            df (Pandas.DataFrame): Dataframe with the instances of the node.\n",
    "            feature (str): Feature to calculate the metric.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Two values, the first one is the value of the feature that\n",
    "                minimizes the metric, and the second one is the metric for that\n",
    "                value.\n",
    "        \"\"\"\n",
    "        value_to_metric = {}\n",
    "        values = list(\n",
    "            set(\n",
    "                list(df[str(feature) + \"_upper\"][self.mask])\n",
    "                + list(df[str(feature) + \"_lower\"][self.mask])\n",
    "            )\n",
    "        )\n",
    "        np.random.shuffle(values)\n",
    "        # values = values[:3]\n",
    "        # print(values)\n",
    "        for value in values:\n",
    "            left_mask = [\n",
    "                True if upper <= value else False\n",
    "                for upper in df[str(feature) + \"_upper\"]\n",
    "            ]\n",
    "            right_mask = [\n",
    "                True if lower >= value else False\n",
    "                for lower in df[str(feature) + \"_lower\"]\n",
    "            ]\n",
    "            both_mask = [\n",
    "                True if value < upper and value > lower else False\n",
    "                for lower, upper in zip(\n",
    "                    df[str(feature) + \"_lower\"], df[str(feature) + \"_upper\"]\n",
    "                )\n",
    "            ]\n",
    "            value_to_metric[value] = self.get_value_metric(\n",
    "                df, left_mask, right_mask, both_mask\n",
    "            )\n",
    "        # print('CHECK FEATURE SPLIT VALUE')\n",
    "        # print(value_to_metric)\n",
    "        val = min(value_to_metric, key=value_to_metric.get)\n",
    "        return val, value_to_metric[val]\n",
    "\n",
    "    def get_value_metric(self, df, left_mask, right_mask, both_mask):\n",
    "        \"\"\"Function that calculates the metric for a given value of a feature.\n",
    "\n",
    "        Args:\n",
    "            df (Pandas.DataFrame): Dataframe with the instances of the node.\n",
    "            value_mask (Pandas.DataFrame): Masked dataframe with the instances of the\n",
    "                node for a given value of a feature.\n",
    "            feature_mask (Pandas.DataFrame): Masked dataframe with the instances of the\n",
    "                node for a given feature.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        l_df_mask = np.logical_and(np.logical_or(left_mask, both_mask), self.mask)\n",
    "        r_df_mask = np.logical_and(np.logical_or(right_mask, both_mask), self.mask)\n",
    "        if np.sum(l_df_mask) == 0 or np.sum(r_df_mask) == 0:\n",
    "            return np.inf\n",
    "        l_entropy, r_entropy = self.calculate_entropy(\n",
    "            df, l_df_mask\n",
    "        ), self.calculate_entropy(df, r_df_mask)\n",
    "        l_prop = np.sum(l_df_mask) / len(l_df_mask)\n",
    "        r_prop = np.sum(r_df_mask) / len(l_df_mask)\n",
    "        return l_entropy * l_prop + r_entropy * r_prop\n",
    "\n",
    "    def predict_probas_and_depth(self, inst, training_df, explanation=None):\n",
    "        \"\"\"This function returns the prediction of the instance and the depth of the\n",
    "        tree that has been used to predict the instance. Also returns the explanation,\n",
    "        that is calculated as the feature and the value of the feature that has been\n",
    "        used to predict the instance.\n",
    "\n",
    "        Args:\n",
    "            inst (np.array): Instance to be predicted\n",
    "            training_df (Pandas.DataFrame): Pandas dataframe with the training data.\n",
    "\n",
    "        Returns:\n",
    "            prediction: The prediction for a node\n",
    "        \"\"\"\n",
    "        if explanation is None:\n",
    "            explanation = {}\n",
    "        if self.left is None and self.right is None:\n",
    "            return self.node_probas(training_df), 1, {}\n",
    "        if inst[self.split_feature] <= self.split_value:\n",
    "            prediction, depth, aux_explanation = self.left.predict_probas_and_depth(\n",
    "                inst, training_df\n",
    "            )\n",
    "            explanation.update(aux_explanation)\n",
    "            explanation[f\"x{self.split_feature}\"] = \"<=\" + str(self.split_value)\n",
    "            return prediction, depth + 1, explanation\n",
    "        else:\n",
    "            prediction, depth, aux_explanation = self.right.predict_probas_and_depth(\n",
    "                inst, training_df\n",
    "            )\n",
    "            explanation.update(aux_explanation)\n",
    "            explanation[f\"x{self.split_feature}\"] = \">\" + str(self.split_value)\n",
    "            return prediction, depth + 1, explanation\n",
    "\n",
    "    def node_probas(self, df):\n",
    "        \"\"\"Function that get the probabilities for a node. Those probabilities are\n",
    "        the label of the node.\n",
    "\n",
    "        Args:\n",
    "            df (Pandas.DataFrame): Dataframe with the instances of the node.\n",
    "\n",
    "        Returns:\n",
    "            array: Array with the probabilities for each class.\n",
    "        \"\"\"\n",
    "        x = df[\"probas\"][self.mask].mean()\n",
    "        return x / x.sum()\n",
    "\n",
    "    \"\"\"\n",
    "    def get_node_prediction(self,training_df):\n",
    "        v=training_df['probas'][self.mask][0]\n",
    "        v=[i/np.sum(v) for i in v]\n",
    "        return np.array(v)\n",
    "    \n",
    "    \n",
    "    def opposite_col(self,s):\n",
    "        if 'upper' in s:\n",
    "            return s.replace('upper','lower')\n",
    "        else:\n",
    "            return s.replace('lower', 'upper')\n",
    "    \"\"\"\n",
    "\n",
    "    def calculate_entropy(self, test_df, test_df_mask):\n",
    "        \"\"\"Function that calculates the entropy for a given node.\n",
    "\n",
    "        Args:\n",
    "            df (Pandas.DataFrame): Dataframe with the instances of the node.\n",
    "            df_mask (Pandas.DataFrame): Masked dataframe with the instances of the node.\n",
    "\n",
    "        Returns:\n",
    "            float: The entropy of the node.\n",
    "        \"\"\"\n",
    "        x = test_df[\"probas\"][test_df_mask].mean()\n",
    "        return entropy(x / x.sum())\n",
    "\n",
    "    def count_depth(self):\n",
    "        \"\"\"Function that counts the depth of a node.\n",
    "\n",
    "        Returns:\n",
    "            int: The depth of the node.\n",
    "        \"\"\"\n",
    "        if self.right is None:\n",
    "            return 1\n",
    "        return max(self.left.count_depth(), self.right.count_depth()) + 1\n",
    "\n",
    "    def number_of_children(self):\n",
    "        \"\"\"\n",
    "        Function that returns the number of children of a node.\n",
    "        Returns:\n",
    "            int: The number of children of a node.\n",
    "        \"\"\"\n",
    "        if self.right is None:\n",
    "            return 1\n",
    "        return 1 + self.right.number_of_children() + self.left.number_of_children()\n",
    "\n",
    "    \"\"\"\n",
    "    def has_same_class(self,df):\n",
    "        labels=set([np.argmax(l) for l in df['probas'][self.mask]])\n",
    "        if len(labels)>1:\n",
    "            return False\n",
    "        return True\n",
    "    \"\"\"\n",
    "\n",
    "    ################################## NEW ####################################\n",
    "    def new_model_measures(self, X, Y, branches_df, classes_p=None):\n",
    "        # DEPRECATED\n",
    "        # NO LAS ESTOY UTILIZANDO\n",
    "        result_dict = {}\n",
    "        probas, depths = [], []\n",
    "        self.classes_ = classes_p if classes_p is not None else self.classes_\n",
    "        for inst in X:\n",
    "            prob, depth = self.predict_probas_and_depth(inst, branches_df)\n",
    "            probas.append(prob)\n",
    "            depths.append(depth)\n",
    "        print(self.classes_)\n",
    "        # Modificar la predicción, para que se haga sobre la clase más probable sobre las que tiene el cliente\n",
    "        # Modificar para que funcione correctamente sobre multiclase\n",
    "        predictions = [\n",
    "            self.classes_[i] for i in np.array([np.argmax(prob) for prob in probas])\n",
    "        ]\n",
    "        result_dict[\"new_model_average_depth\"] = np.mean(depths)\n",
    "        result_dict[\"new_model_min_depth\"] = np.min(depths)\n",
    "        result_dict[\"new_model_max_depth\"] = np.max(depths)\n",
    "        result_dict[\"new_model_accuracy\"] = np.sum(predictions == Y) / len(Y)\n",
    "        result_dict[\"new_model_auc\"] = self.get_auc(Y, np.array(probas), self.classes_)\n",
    "        result_dict[\"new_model_kappa\"] = cohen_kappa_score(Y, predictions)\n",
    "        result_dict[\"new_model_number_of_nodes\"] = self.number_of_children()\n",
    "        result_dict[\"new_model_probas\"] = probas\n",
    "        result_dict[\"predictions\"] = predictions\n",
    "        return result_dict\n",
    "\n",
    "    def predict(self, X, classes_, branches_df):\n",
    "        \"\"\"Function that predicts the class for a given instance.\n",
    "\n",
    "        Args:\n",
    "            X (array): instance to predict\n",
    "            classes_ (array): classes of the dataset\n",
    "            branches_df (Pandas.DataFrame): Dataframe with the branches of the tree.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Predictions and explanations for the instances predicted.\n",
    "        \"\"\"\n",
    "        probas, depths = [], []\n",
    "        explanations = []\n",
    "        classes_ = self.classes_ if self.classes_ is not None else classes_\n",
    "        for inst in X:\n",
    "            prob, depth, explanation = self.predict_probas_and_depth(inst, branches_df)\n",
    "            probas.append(prob)\n",
    "            depths.append(depth)\n",
    "            explanations.append(explanation)\n",
    "        predictions = [\n",
    "            classes_[i] for i in np.array([np.argmax(prob) for prob in probas])\n",
    "        ]\n",
    "        explanations = [\n",
    "            self.generate_explanation(pred, expl)\n",
    "            for pred, expl in zip(predictions, explanations)\n",
    "        ]\n",
    "        return predictions, explanations\n",
    "\n",
    "    def predict_probas(self, X, classes_, branches_df):\n",
    "        \"\"\"Function that predicts the class for a given instance.\n",
    "\n",
    "        Args:\n",
    "            X (array): instance to predict.\n",
    "            classes_ (array): classes of the dataset.\n",
    "            branches_df (Pandas.DataFrame): Dataframe with the branches of the tree.\n",
    "\n",
    "        Returns:\n",
    "            array: Returns an array containing the probabilities for each class.\n",
    "        \"\"\"\n",
    "        probas, depths = [], []\n",
    "        explanations = []\n",
    "        classes_ = self.classes_ if self.classes_ is not None else classes_\n",
    "        for inst in X:\n",
    "            # prob, depth = self.predict_probas_and_depth(inst, branches_df)\n",
    "            prob, depth, explanation = self.predict_probas_and_depth(inst, branches_df)\n",
    "            probas.append(prob)\n",
    "            depths.append(depth)\n",
    "            explanations.append(explanation)\n",
    "        return np.array(probas), explanations\n",
    "\n",
    "    def get_auc(self, Y, y_score, classes):\n",
    "        \"\"\"Function to calculate the auc for a given set of predictions.\n",
    "\n",
    "        Args:\n",
    "            Y (array): Labels of the instances.\n",
    "            y_score (array): Array with the predictions for each instance.\n",
    "            classes (array): Array with the classes of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        y_test_binarize = np.array([[1 if i == c else 0 for c in classes] for i in Y])\n",
    "        fpr, tpr, _ = roc_curve(y_test_binarize.ravel(), y_score.ravel())\n",
    "        return auc(fpr, tpr)\n",
    "\n",
    "    def generate_explanation(self, target, explanation):\n",
    "        \"\"\"Function that generates an explanation for a given target instance.\n",
    "\n",
    "        Args:\n",
    "            target (int): Label predicted for an instance\n",
    "            explanation (Dict): Dict containing the explanation for the instance\n",
    "            in format feature: '<=value' or feature: '>value'.\n",
    "        \"\"\"\n",
    "        ret = \"\"\n",
    "        ret += f\"The instance was classified as {target}. \"\n",
    "        ret += \"Because:\"\n",
    "        ret += \"\".join(\n",
    "            [f\" {feature}{value},\" for feature, value in explanation.items()]\n",
    "        )\n",
    "        return f\"{ret[:-1]}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第6-9步: 聚合规则并构建全局模型...\n",
      "Estimators: None\n",
      "Iteration 1: 15 conjunctions\n",
      "\n",
      "Las reglas actuales son: \n",
      "Iteration 2: 41 conjunctions\n",
      "\n",
      "Las reglas actuales son: \n",
      "Conjunction set length: 117\n",
      "Conjunction set length after removing duplicates: 117\n",
      "branches df aggreagator is not null:      0_upper  0_lower  1_upper  1_lower  2_upper  2_lower  3_upper  3_lower  \\\n",
      "0       30.5     -inf   7139.5     -inf   2218.0     -inf     40.5     -inf   \n",
      "1       30.5     -inf   7139.5     -inf   2218.0     -inf     40.5     -inf   \n",
      "2        inf     30.5   7139.5     -inf   2218.0     -inf     40.5     -inf   \n",
      "3        inf     30.5   7139.5     -inf   2218.0     -inf     40.5     -inf   \n",
      "4       30.5     -inf   7139.5     -inf   2391.5   2218.0     40.5     -inf   \n",
      "..       ...      ...      ...      ...      ...      ...      ...      ...   \n",
      "112      inf     -inf   5095.5     -inf   1794.0     -inf      inf     39.0   \n",
      "113      inf     -inf   7032.5   5095.5   1794.0     -inf      inf     -inf   \n",
      "114      inf     -inf   5095.5     -inf      inf   1794.0      inf     -inf   \n",
      "115      inf     -inf   7032.5   5095.5      inf   1794.0      inf     -inf   \n",
      "116      inf     -inf      inf   7032.5      inf     -inf      inf     -inf   \n",
      "\n",
      "     4_upper  4_lower  ...  104_lower  105_upper  105_lower  106_upper  \\\n",
      "0        inf     -inf  ...       -inf        inf       -inf        inf   \n",
      "1        inf     -inf  ...       -inf        inf       -inf        inf   \n",
      "2        inf     -inf  ...       -inf        inf       -inf        inf   \n",
      "3        inf     -inf  ...       -inf        inf       -inf        inf   \n",
      "4        inf     -inf  ...       -inf        inf       -inf        inf   \n",
      "..       ...      ...  ...        ...        ...        ...        ...   \n",
      "112      inf     -inf  ...       -inf        inf       -inf        inf   \n",
      "113      inf     -inf  ...       -inf        inf       -inf        inf   \n",
      "114      inf     -inf  ...       -inf        inf       -inf        inf   \n",
      "115      inf     -inf  ...       -inf        inf       -inf        inf   \n",
      "116      inf     -inf  ...       -inf        inf       -inf        inf   \n",
      "\n",
      "     106_lower  107_upper  107_lower  number_of_samples  branch_probability  \\\n",
      "0         -inf        inf       -inf               1077            0.020670   \n",
      "1         -inf        inf       -inf                213            0.020670   \n",
      "2         -inf        inf       -inf               1085            0.020670   \n",
      "3         -inf        inf       -inf                161            0.020670   \n",
      "4         -inf        inf       -inf                258            0.020670   \n",
      "..         ...        ...        ...                ...                 ...   \n",
      "112       -inf        inf       -inf                345            0.005317   \n",
      "113       -inf        inf       -inf                104            0.005308   \n",
      "114       -inf        inf       -inf                 55            0.000248   \n",
      "115       -inf        inf       -inf                 66            0.000316   \n",
      "116       -inf        inf       -inf                 71            0.000316   \n",
      "\n",
      "                                          probas  \n",
      "0     [0.9894650592523329, 0.010534940747667119]  \n",
      "1     [0.9727983925856662, 0.027201607414333785]  \n",
      "2      [0.9643037689297522, 0.03569623107024776]  \n",
      "3      [0.8129944710170387, 0.18700552898296124]  \n",
      "4       [0.8446043165467626, 0.1553956834532374]  \n",
      "..                                           ...  \n",
      "112    [0.32184400226151966, 0.6781559977384803]  \n",
      "113     [0.0876923076923077, 0.9123076923076923]  \n",
      "114     [0.0401213282247765, 0.9598786717752235]  \n",
      "115   [0.009259259259259259, 0.9907407407407407]  \n",
      "116  [0.0035714285714285713, 0.9964285714285714]  \n",
      "\n",
      "[117 rows x 219 columns]\n",
      "全局模型构建完成\n"
     ]
    }
   ],
   "source": [
    "# 4. 规则聚合函数\n",
    "def aggregate_rules(client_models, selected_indices):\n",
    "    \"\"\"聚合所选客户端的规则\"\"\"\n",
    "    # 仅保留所选树的规则\n",
    "    selected_models = [client_models[i] for i in selected_indices]\n",
    "    \n",
    "    # 提取规则和类别\n",
    "    client_branches = [model['local_branches'] for model in selected_models]\n",
    "    client_classes = [model['local_classes'] for model in selected_models]\n",
    "    client_branches_df = [model['local_branches_df'] for model in selected_models]\n",
    "    model_types = ['cart'] * len(selected_models)  # 简化为只使用CART\n",
    "    \n",
    "    \n",
    "    # 准备输入格式\n",
    "    list_of_weights = [(branches, classes, branches_df, model_type) \n",
    "                       for branches, classes, branches_df, model_type in \n",
    "                       zip(client_branches, client_classes, client_branches_df, model_types)]\n",
    "    \n",
    "    # 提取所有类别和特征\n",
    "    classes_ = set()\n",
    "    for client_class in client_classes:\n",
    "        classes_ |= set(client_class)\n",
    "    classes_ = list(classes_)\n",
    "    \n",
    "    # 提取分支列表\n",
    "    client_cs = [cs for cs in client_branches]\n",
    "    \n",
    "    # 聚合为全局模型\n",
    "    global_model = generate_cs_dt_branches_from_list(client_cs, classes_, TreeBranch)\n",
    "    \n",
    "    return global_model\n",
    "\n",
    "# 7. 聚合规则并构建全局模型\n",
    "print(\"\\n第6-9步: 聚合规则并构建全局模型...\")\n",
    "global_model = aggregate_rules(client_models, selected_indices)\n",
    "print(\"全局模型构建完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 全局模型评估函数\n",
    "def evaluate_global_model(global_model, test_data):\n",
    "    \"\"\"评估全局聚合模型的性能\"\"\"\n",
    "    X_test, y_test = test_data.to_numpy()\n",
    "    \n",
    "    # 从全局模型中获取分支类别\n",
    "    branches_df = global_model[2]\n",
    "    classes_tree = get_classes_branches(branches_df)\n",
    "    \n",
    "    # 使用全局模型进行预测\n",
    "    y_pred, _ = global_model[1].predict(X_test, classes_tree, branches_df)\n",
    "    \n",
    "    # 计算性能指标\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    print(\"\\n全局模型在测试集上的性能:\")\n",
    "    print(f\"准确率: {acc:.4f}\")\n",
    "    print(f\"宏平均F1: {f1:.4f}\")\n",
    "    print(f\"分类报告: \\n{report}\")\n",
    "    \n",
    "    return acc, f1\n",
    "\n",
    "def get_classes_branches(branches):\n",
    "    \"\"\"从分支DataFrame中获取类别\"\"\"\n",
    "    assert branches is not None\n",
    "    return list(range(len(branches['probas'].iloc[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第10步: 评估全局模型...\n",
      "\n",
      "全局模型在测试集上的性能:\n",
      "准确率: 0.8543\n",
      "宏平均F1: 0.7769\n",
      "分类报告: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.95      0.91      7392\n",
      "           1       0.79      0.54      0.65      2377\n",
      "\n",
      "    accuracy                           0.85      9769\n",
      "   macro avg       0.83      0.75      0.78      9769\n",
      "weighted avg       0.85      0.85      0.84      9769\n",
      "\n",
      "\n",
      "--- ICDTA4FL 简化Demo完成 ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 8. 评估全局模型\n",
    "print(\"\\n第10步: 评估全局模型...\")\n",
    "eval_results = evaluate_global_model(global_model, test_data)\n",
    "    \n",
    "print(\"\\n--- ICDTA4FL 简化Demo完成 ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
